{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW9cja3OOY1M"
      },
      "source": [
        "# Part 1: Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu3lk7_sQu8W"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community pypdf sentence-transformers tiktoken rank_bm25 langchain-together tavily-python langgraph gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv4wBgkaZNbN"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu -q\n",
        "#!pip install faiss-cpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqq6k14OXfN"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import urllib.parse\n",
        "\n",
        "page_url = 'https://stanford.edu/~jurafsky/slp3/'\n",
        "page_dom = BeautifulSoup(requests.get(page_url).content, 'html.parser')\n",
        "\n",
        "chapters = []\n",
        "selected_chapters = ['13', '14', '15', '16']\n",
        "\n",
        "for pdf_anchor in filter(lambda a: a['href'].split('.pdf')[0].isnumeric(), page_dom.findAll('a')):\n",
        "  if pdf_anchor['href'].split('.pdf')[0] not in selected_chapters:\n",
        "    continue\n",
        "\n",
        "  pdf_url = urllib.parse.urljoin(page_url, pdf_anchor['href'])\n",
        "  chapters.append({'title': pdf_anchor.text, 'url': pdf_url})\n",
        "\n",
        "print(chapters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6RQvEtgQpQA"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders.merge import MergedDataLoader\n",
        "\n",
        "loaders = [PyPDFLoader(chapter['url']) for chapter in chapters]\n",
        "loader_all = MergedDataLoader(loaders=loaders)\n",
        "docs = loader_all.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX5zs1WjSqtB"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJNMxGQAUbAk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziI3007aU7uQ"
      },
      "source": [
        "# Part 2: Embedding and store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j72fjLzkTGm5"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings(show_progress=True, multi_process=True)\n",
        "\n",
        "store = LocalFileStore(\"/content/drive/MyDrive/CachedEmbeddings/\")\n",
        "\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(embedding_function, store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDK4xepeUsSx"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.from_documents(documents=chunks, embedding=embedding_function) #FAISS.from_documents(documents=chunks, embedding=cached_embedder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyV4XjtKZeF5"
      },
      "source": [
        "# Part 3: Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojB0Fh15ZIt3"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(chunks, search_kwargs={\"k\": 3})\n",
        "faiss_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.3, 0.7]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m1TgNkbm8md"
      },
      "outputs": [],
      "source": [
        "sample_questions = [\"What is a lexical gap?\", \"Why are Binary Trees important?\", \"Who is the president of bolivia?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC-iq9YBb1pQ"
      },
      "outputs": [],
      "source": [
        "for question in sample_questions:\n",
        "  print(f'Question: {question}')\n",
        "  print(ensemble_retriever.invoke(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38JyCiFRb75U"
      },
      "source": [
        "# Part 4: Router chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lzi3cl2cilX"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc7NzYnBcSw9"
      },
      "outputs": [],
      "source": [
        "from langchain_together import ChatTogether\n",
        "\n",
        "llm = ChatTogether(\n",
        "  together_api_key=userdata.get('together_api_key'),\n",
        "  model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "  temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBMZxDZLdim0"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "router_prompt_template = (\n",
        "    \"You are an expert in routing user queries to either a `VectorStore` or a `SearchEngine` or `None` depending on the conditions below: .\\n\"\n",
        "    \"If the query is *NOT RELATED* to Computer Science or NLP, choose `None`.\\n\"\n",
        "    \"If the query is *RELATED* to Natural Language Processing Applications (Machine Translation, Question Answering and Information Retrieval, Chatbots and Dialogue Systems, Automatic Speech Recognition and Text-to-Speech), choose `VectorStore`.\\n\"\n",
        "    \"If the query is *RELATED* to Computer Science (Algorithms and Data Structures, Computer Networks, Database Systems, Operating Systems, Artificial Intelligence and Machine Learning, Computer Security, Software Engineering, etc), choose `SearchEngine`.\\n\"\n",
        "    \"Again, If the query is *NOT RELATED* to Computer Science or NLP, choose `None`.\\n\"\n",
        "    \"Return only and only the name of the tool you chose and *nothing more*.\\n\"\n",
        "    \"{output_instructions}\\n\"\n",
        "    \"Query: {query}\\n\"\n",
        ")\n",
        "\n",
        "router_prompt = ChatPromptTemplate.from_template(\n",
        "    template=router_prompt_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC-GmYLSfKJa"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "class ChosenTool(BaseModel):\n",
        "    tool_name: Literal[\"None\", \"VectorStore\", \"SearchEngine\"] = Field(description=\"Chosen tool by LLM in question routing.\")\n",
        "\n",
        "question_router_parser = PydanticOutputParser(pydantic_object=ChosenTool)\n",
        "question_router_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw55ivACmcfz"
      },
      "outputs": [],
      "source": [
        "chain_router = router_prompt | llm | question_router_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFHneD9em7Ee"
      },
      "outputs": [],
      "source": [
        "for question in sample_questions:\n",
        "  print(f'Question: {question}')\n",
        "  print(chain_router.invoke({\"query\": question, \"output_instructions\": question_router_parser.get_format_instructions()}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYGA5JZUggLZ"
      },
      "source": [
        "# Part 5: Search Engine chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eybyhRyvgFWj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search = TavilySearchAPIWrapper(tavily_api_key=userdata.get(\"TAVILY_API_KEY\"))\n",
        "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5, search_depth=\"advanced\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaQMXZ7QjTMV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def search_post_process_func(results):\n",
        "    documents = []\n",
        "\n",
        "    for result in results:\n",
        "      documents.append(Document(\n",
        "        page_content=result['content'],\n",
        "        metadata={\"source\": result['url']}\n",
        "      ))\n",
        "\n",
        "    return documents\n",
        "\n",
        "search_post_process = RunnableLambda(search_post_process_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5bvmOhgiitl"
      },
      "outputs": [],
      "source": [
        "chain_engine_search = tavily_tool | search_post_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms2nvCkkohvC"
      },
      "outputs": [],
      "source": [
        "sample_documents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzPg6OidoEEx"
      },
      "outputs": [],
      "source": [
        "for question in sample_questions:\n",
        "  print(f'Question: {question}')\n",
        "  results = chain_engine_search.invoke(question)\n",
        "  print(results)\n",
        "\n",
        "  sample_documents.append(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE1gUsK8kgmq"
      },
      "source": [
        "# Part 6: Relevancy Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z36jW7HYkgG_"
      },
      "outputs": [],
      "source": [
        "relevancy_check_template = (\n",
        "    \"You are an expert in determining whether a document is relevant to a user query or not.\\n\"\n",
        "    \"If the document is related to the query return only and only `Relevant`.\\n\"\n",
        "    \"If the document is NOT related to the query return only and only `Irrelevant`.\\n\"\n",
        "    \"A relevant document will help the user to understand their questions better or answer them.\\n\"\n",
        "    \"A relevant document will guide a helpful assistant in answering a user's query and will it discuss the specific topic of the user query.\\n\"\n",
        "    \"{output_instructions}\\n\"\n",
        "    \"Document: ```{document}```\\n\"\n",
        "    \"Query: ```{query}```\\n\"\n",
        ")\n",
        "\n",
        "relevancy_check_prompt = ChatPromptTemplate.from_template(\n",
        "    template=relevancy_check_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy13X1h5mAvI"
      },
      "outputs": [],
      "source": [
        "class Relevance(BaseModel):\n",
        "    relevance: Literal[\"Relevant\", \"Irrelevant\"] = Field(description=\"Relevancy determined by the LLM.\")\n",
        "\n",
        "relevancy_check_parser = PydanticOutputParser(pydantic_object=Relevance)\n",
        "relevancy_check_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bk4kBL2mWfG"
      },
      "outputs": [],
      "source": [
        "chain_check_relevancy = relevancy_check_prompt | llm | relevancy_check_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYK5nHmEoXMq"
      },
      "outputs": [],
      "source": [
        "for question, document in zip(sample_questions, sample_documents):\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Document: {document}')\n",
        "\n",
        "  print(chain_check_relevancy.invoke({\"query\": question, \"document\": document,\"output_instructions\": relevancy_check_parser.get_format_instructions()}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My41rLr1pCPm"
      },
      "source": [
        "# Part 7: Fallback chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPpetGU9pAMD"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "fallback_prompt = ChatPromptTemplate.from_template(\n",
        "    (\n",
        "        \"You are a friendly assistant to assist in related subjects below:.\\n\"\n",
        "        \"1. Natural Language Processing (Machine Translation, Question Answering, Information Retrieval, Text Classification, Sentiment Analysis, Named Entity Recognition, Part-of-Speech Tagging, Dependency Parsing, Neural Networks, Language Models, etc)\\n\"\n",
        "        \"2. Computer Science (Algorithms and Data Structures, Computer Networks, Database Systems, Operating Systems, Artificial Intelligence and Machine Learning, Computer Security, Software Engineering, etc)\\n\"\n",
        "        \"Do not respond to queries that are not related.\\n\"\n",
        "        \"If a query is not related to NLP or Computer Science, acknowledge your limitations.\\n\"\n",
        "        \"Provide concise responses to only related queries.\\n\\n\"\n",
        "        \"Current conversations:\\n\\n{chat_history}\\n\\n\"\n",
        "        \"human: {query}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "fallback_chain = (\n",
        "    {\n",
        "        \"chat_history\": lambda x: \"\\n\".join(\n",
        "            [\n",
        "                (\n",
        "                    f\"User: {msg.content}\"\n",
        "                    if isinstance(msg, HumanMessage)\n",
        "                    else f\"Assistant: {msg.content}\"\n",
        "                )\n",
        "                for msg in x[\"chat_history\"]\n",
        "            ]\n",
        "        ),\n",
        "        \"query\": itemgetter(\"query\") ,\n",
        "    }\n",
        "    | fallback_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCFHfamDrWy5"
      },
      "source": [
        "# Part 8: Generate with context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STY9RI2ZraLQ"
      },
      "outputs": [],
      "source": [
        "generate_with_context_template = (\n",
        "    \"You are a helpful assistant. Answer the query below based only on the provided context. If the given context is not relevant, DO NOT answer based on your own knowledge\\n\\n\"\n",
        "    \"context: {context}\\n\\n\"\n",
        "    \"query: {query}\"\n",
        ")\n",
        "\n",
        "generate_with_context_prompt = ChatPromptTemplate.from_template(generate_with_context_template)\n",
        "generate_with_context_chain = generate_with_context_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7655FfFqrnoK"
      },
      "outputs": [],
      "source": [
        "query = \"What is a lexical gap?\"\n",
        "context = ensemble_retriever.invoke(query)\n",
        "response = generate_with_context_chain.invoke({\"query\": query, \"context\": context})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpFIL9CLsBTq"
      },
      "source": [
        "# Part 9: LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_veN_WbTr_w-"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages.base import BaseMessage\n",
        "import operator\n",
        "\n",
        "class AgentSate(TypedDict):\n",
        "    \"\"\"The dictionary keeps track of the data required by the various nodes in the graph\"\"\"\n",
        "\n",
        "    query: str\n",
        "    chat_history: list[BaseMessage]\n",
        "    generation: str\n",
        "    documents: list[Document]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOEFnEvvt3PE"
      },
      "outputs": [],
      "source": [
        "def router_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    response = chain_router.invoke({\"query\": query, \"output_instructions\": question_router_parser.get_format_instructions()})\n",
        "    chosen_tool = response.tool_name.lower()\n",
        "\n",
        "    if chosen_tool == \"vectorstore\":\n",
        "        print(f\"Tool: vector_store\")\n",
        "        return \"vector_store\"\n",
        "\n",
        "    if chosen_tool == \"searchengine\":\n",
        "        print(f\"Tool: search_engine\")\n",
        "        return \"search_engine\"\n",
        "\n",
        "    print(f\"Tool: fallback\")\n",
        "    return \"fallback\"\n",
        "\n",
        "def retrieve_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    documents = ensemble_retriever.invoke(query)\n",
        "\n",
        "    print(f'Number of retrieved documents: {len(documents)}')\n",
        "\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "def search_engine_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    documents = chain_engine_search.invoke(query)\n",
        "\n",
        "    print(f'Number of searched documents: {len(documents)}')\n",
        "\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "def fallback_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    chat_history = state[\"chat_history\"]\n",
        "    generation = fallback_chain.invoke({\"query\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    return {\"generation\": generation}\n",
        "\n",
        "\n",
        "def generate_with_context_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    print(f'Query: {query}')\n",
        "    print(f'Documents: {documents}')\n",
        "\n",
        "    generation = generate_with_context_chain.invoke({\"query\": query, \"context\": documents})\n",
        "    return {\"generation\": generation}\n",
        "\n",
        "def filter_documents_node(state: dict):\n",
        "  query = state[\"query\"]\n",
        "  documents = state[\"documents\"]\n",
        "  filtered_documents = []\n",
        "\n",
        "  for document in documents:\n",
        "    response = chain_check_relevancy.invoke({\"query\": query, \"document\": document,\n",
        "                                  \"output_instructions\": relevancy_check_parser.get_format_instructions()})\n",
        "\n",
        "    relevancy = response.relevance.lower()\n",
        "\n",
        "    if relevancy == 'relevant':\n",
        "      print(f'Relavent document: {document}')\n",
        "      filtered_documents.append(document)\n",
        "    else:\n",
        "      print(f'Irrelavent document: {document}')\n",
        "\n",
        "  return {\"documents\": filtered_documents}\n",
        "\n",
        "def continue_with_generation(state: dict):\n",
        "  print(f'Number of relavent documents: {len(state[\"documents\"])}')\n",
        "\n",
        "  if len(state['documents']) > 0:\n",
        "    return 'generate_with_context'\n",
        "  else:\n",
        "    return 'search_engine'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es7ifqCqueXt"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentSate)\n",
        "\n",
        "workflow.add_node(\"vector_store\", retrieve_node)\n",
        "workflow.add_node(\"search_engine\", search_engine_node)\n",
        "workflow.add_node(\"fallback\", fallback_node)\n",
        "workflow.add_node(\"generate_with_context\", generate_with_context_node)\n",
        "workflow.add_node(\"filter_documents\", filter_documents_node)\n",
        "\n",
        "workflow.set_conditional_entry_point(\n",
        "    router_node,\n",
        "    {\n",
        "        \"fallback\": \"fallback\",\n",
        "        \"vector_store\": \"vector_store\",\n",
        "        \"search_engine\": \"search_engine\"\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"vector_store\", \"filter_documents\")\n",
        "workflow.add_edge(\"search_engine\", \"filter_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"filter_documents\",\n",
        "    continue_with_generation,\n",
        "    {\n",
        "        \"search_engine\": \"search_engine\",\n",
        "        \"generate_with_context\": \"generate_with_context\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"generate_with_context\", END)\n",
        "workflow.add_edge(\"fallback\", END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJrnQIVwvnbd"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile(debug=False)\n",
        "plot = app.get_graph().draw_mermaid_png()\n",
        "\n",
        "with open(\"plot.png\", \"wb\") as fp:\n",
        "    fp.write(plot)\n",
        "\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "img = Image.open(BytesIO(plot))\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErESOWXG2I5v"
      },
      "outputs": [],
      "source": [
        "response = app.invoke({\"query\": \"Who is the president of Iran?\", \"chat_history\": []})\n",
        "print(response[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHj0taDp6V2H"
      },
      "outputs": [],
      "source": [
        "response = app.invoke({\"query\": \"What is RAG (Retrieval-Augmented Generation)?\", \"chat_history\": []})\n",
        "print(response[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q6yLRCo-JYX"
      },
      "outputs": [],
      "source": [
        "response = app.invoke({\"query\": \"Explain the knapsack problem\", \"chat_history\": []})\n",
        "print(response[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJsN9cO7-Zos"
      },
      "outputs": [],
      "source": [
        "response = app.invoke({\"query\": \"What is the huggingface library intended to do?\", \"chat_history\": []})\n",
        "print(response[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIG9Wk3e-x0M"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from uuid import uuid4\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "history = {}\n",
        "session_id = str(uuid4())\n",
        "\n",
        "def chat(query):\n",
        "\n",
        "    # Initialize the chat history for the current session\n",
        "    if session_id not in history:\n",
        "        history[session_id] = []\n",
        "\n",
        "    chat_history = history[session_id]\n",
        "\n",
        "    # Invoke the app with the current query and chat history\n",
        "    result = app.invoke({\"query\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    # Separate the response from the retrieved documents\n",
        "    response = result[\"generation\"]\n",
        "    documents = result.get(\"documents\")\n",
        "\n",
        "    # Add the current exchange to the chat history\n",
        "    chat_history.extend([HumanMessage(content=query), AIMessage(content=response)])\n",
        "\n",
        "    if not documents:\n",
        "        return response, documents\n",
        "\n",
        "    documents = [\n",
        "        f\"{doc.page_content}\\nsource: {doc.metadata['source']}\" for doc in documents\n",
        "    ]\n",
        "\n",
        "    return response, \"\\n\\n\".join(documents)\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chat,\n",
        "    inputs=gr.Textbox(label=\"Question\"),\n",
        "    outputs=[gr.Textbox(label=\"Response\"), gr.Textbox(label=\"Retrieved Documents\")],\n",
        "    title=\"RAG Chatbot\",\n",
        "    description=\"Ask a Computer Science or NLP related query and the chatbot will generate a response using Retrieval Augmented Generation.\",\n",
        ")\n",
        "\n",
        "demo.launch(share=True, inline=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}